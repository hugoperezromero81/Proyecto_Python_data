{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ca6b8e5b",
   "metadata": {},
   "source": [
    "# üßπ 02_Limpieza de Datos\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a33b225",
   "metadata": {},
   "source": [
    "\n",
    "En este notebook realizamos las tareas de limpieza necesarias para preparar los datasets \n",
    "**bank** y **customers** para el an√°lisis exploratorio y modelos posteriores."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5412ff51",
   "metadata": {},
   "source": [
    "## Importaci√≥n de librer√≠as\n",
    "Cargamos las librer√≠as necesarias:  \n",
    "- **pandas**: para manipulaci√≥n de datos  \n",
    "- **numpy**: para valores nulos y operaciones num√©ricas  \n",
    "- **datetime**: para trabajar con variables de fecha\n",
    "- **os**: para trabajar con archivos y directorios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59d04a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Paso 0: Importaci√≥n de librer√≠as\n",
    "# ==============================\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Mostrar todas las columnas en pantalla\n",
    "pd.set_option(\"display.max_columns\", None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad0b8e1",
   "metadata": {},
   "source": [
    "## Carga de datos\n",
    "- El archivo **bank-additional.csv** contiene los datos de campa√±as bancarias.  \n",
    "- El archivo **customer-details.xlsx** contiene 3 hojas con la misma estructura de columnas.  \n",
    "  Para simplificar el an√°lisis, **unificamos todas las hojas en un √∫nico DataFrame**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "075c89a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clientes unificados desde todas las hojas: (43170, 6)\n",
      "Dimensiones df_bank: (43000, 23)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Definir rutas\n",
    "ruta_customer = r\"C:\\Users\\HUGO\\Desktop\\Data Analyst\\09_PYTHON\\Proyecto_Python_Data\\data\\Archivos_origen\\customer-details.xlsx\"\n",
    "ruta_bank = r\"C:\\Users\\HUGO\\Desktop\\Data Analyst\\09_PYTHON\\Proyecto_Python_Data\\data\\Archivos_origen\\bank-additional.csv\"\n",
    "\n",
    "# Consolidar todas las hojas del Excel en un solo DataFrame\n",
    "xls = pd.ExcelFile(ruta_customer)\n",
    "hojas = xls.sheet_names\n",
    "\n",
    "df_customer = pd.concat(\n",
    "    [pd.read_excel(ruta_customer, sheet_name=hoja, index_col=0) for hoja in hojas],\n",
    "    ignore_index=True\n",
    ")\n",
    "\n",
    "# Cargar CSV\n",
    "df_bank = pd.read_csv(ruta_bank, index_col=0, encoding=\"utf-8\")\n",
    "\n",
    "print(\"Clientes unificados desde todas las hojas:\", df_customer.shape)\n",
    "print(\"Dimensiones df_bank:\", df_bank.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29698eff",
   "metadata": {},
   "source": [
    "## Tratamiento de valores 'unknown'\n",
    "En el dataset `bank` muchas variables categ√≥ricas contienen la categor√≠a `unknown`.\n",
    "Estas representan datos faltantes. Vamos a reemplazarlas por `NaN` para identificarlas \n",
    "y poder decidir si imputarlas o tratarlas como categor√≠a propia.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9a50622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank.replace(\"unknown\", np.nan, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af07de6",
   "metadata": {},
   "source": [
    "## Eliminaci√≥n de columnas irrelevantes\n",
    "En el dataset `customers`, la columna `id` podr√≠a ser solo un identificador y no aporta informaci√≥n.\n",
    "Para evitar errores en caso de que no exista, usamos `errors=\"ignore\"`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71758b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_customer = df_customer.drop(columns=[\"id\"], errors=\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2a28091",
   "metadata": {},
   "source": [
    "## Variables derivadas de fechas\n",
    "De la columna `dt_customer` en `customers` extraemos:\n",
    "- A√±o de alta\n",
    "- Mes de alta\n",
    "- Antig√ºedad en d√≠as\n",
    "Esto nos permitir√° analizar patrones temporales en la base de clientes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33c808d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"dt_customer\" in df_customer.columns:\n",
    "    hoy = datetime.today()\n",
    "    df_customer[\"anio_alta\"] = df_customer[\"dt_customer\"].dt.year\n",
    "    df_customer[\"mes_alta\"] = df_customer[\"dt_customer\"].dt.month\n",
    "    df_customer[\"antiguedad_dias\"] = (hoy - df_customer[\"dt_customer\"]).dt.days\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f0998b",
   "metadata": {},
   "source": [
    "## Detecci√≥n y tratamiento de outliers\n",
    "En variables num√©ricas como `income` (customers) o `duration` (bank), \n",
    "existen valores extremos que pueden distorsionar el an√°lisis.\n",
    "Utilizamos el rango intercuart√≠lico (IQR) para detectarlos y marcarlos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06db93fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def marcar_outliers(df, columna):\n",
    "    Q1 = df[columna].quantile(0.25)\n",
    "    Q3 = df[columna].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    limite_inferior = Q1 - 1.5 * IQR\n",
    "    limite_superior = Q3 + 1.5 * IQR\n",
    "    return df[columna].apply(lambda x: x < limite_inferior or x > limite_superior)\n",
    "\n",
    "# Si existen estas columnas, marcamos outliers\n",
    "if \"income\" in df_customer.columns:\n",
    "    df_customer[\"outlier_income\"] = marcar_outliers(df_customer, \"income\")\n",
    "\n",
    "if \"duration\" in df_bank.columns:\n",
    "    df_bank[\"outlier_duration\"] = marcar_outliers(df_bank, \"duration\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be536306",
   "metadata": {},
   "source": [
    "## Normalizaci√≥n de texto en categ√≥ricas\n",
    "En `bank`, homogenizamos las variables categ√≥ricas para evitar inconsistencias:\n",
    "- Pasamos todo a min√∫sculas.\n",
    "- Eliminamos espacios extra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7b9c4a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_bank.select_dtypes(include=\"object\").columns:\n",
    "    df_bank[col] = df_bank[col].astype(str).str.strip().str.lower()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012a4baa",
   "metadata": {},
   "source": [
    "## Revisi√≥n y eliminaci√≥n de duplicados\n",
    "Tras unificar las tres hojas del archivo `customer-details.xlsx`, es posible que existan\n",
    "registros duplicados de clientes. Tambi√©n revisamos duplicados en el dataset `bank`.  \n",
    "Los eliminamos para evitar sesgos en el an√°lisis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b709b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicados en customers: 0\n",
      "Duplicados en bank: 0\n",
      "Dimensiones despu√©s de eliminar duplicados:\n",
      "Customers: (43170, 6)\n",
      "Bank: (43000, 24)\n"
     ]
    }
   ],
   "source": [
    "# Revisar duplicados\n",
    "print(\"Duplicados en customers:\", df_customer.duplicated().sum())\n",
    "print(\"Duplicados en bank:\", df_bank.duplicated().sum())\n",
    "\n",
    "# Eliminar duplicados\n",
    "df_customer = df_customer.drop_duplicates()\n",
    "df_bank = df_bank.drop_duplicates()\n",
    "\n",
    "print(\"Dimensiones despu√©s de eliminar duplicados:\")\n",
    "print(\"Customers:\", df_customer.shape)\n",
    "print(\"Bank:\", df_bank.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bf180f",
   "metadata": {},
   "source": [
    "## Columnas con baja varianza\n",
    "Identificamos columnas en las que todos los registros tienen el mismo valor o casi el mismo.\n",
    "Estas columnas no aportan informaci√≥n relevante al an√°lisis y se eliminan.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96509935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas con baja varianza en customers: []\n",
      "Columnas con baja varianza en bank: []\n"
     ]
    }
   ],
   "source": [
    "# Detectar columnas con un √∫nico valor\n",
    "baja_varianza_customer = [col for col in df_customer.columns if df_customer[col].nunique() == 1]\n",
    "baja_varianza_bank = [col for col in df_bank.columns if df_bank[col].nunique() == 1]\n",
    "\n",
    "print(\"Columnas con baja varianza en customers:\", baja_varianza_customer)\n",
    "print(\"Columnas con baja varianza en bank:\", baja_varianza_bank)\n",
    "\n",
    "# Eliminar columnas con baja varianza\n",
    "df_customer = df_customer.drop(columns=baja_varianza_customer)\n",
    "df_bank = df_bank.drop(columns=baja_varianza_bank)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33152b53",
   "metadata": {},
   "source": [
    "# üßπ Resultados de la Limpieza de Datos\n",
    "\n",
    "En esta primera fase del proyecto hemos realizado una serie de transformaciones y verificaciones sobre los datasets **customers** y **bank**. A continuaci√≥n, se resume lo realizado y los hallazgos principales:\n",
    "\n",
    "### 1. Carga de datos\n",
    "- El archivo **customer-details.xlsx** conten√≠a tres hojas con la misma estructura.  \n",
    "  ‚úÖ Se consolidaron en un √∫nico DataFrame para evitar duplicidad de an√°lisis.  \n",
    "- El archivo **bank-additional.csv** se carg√≥ correctamente utilizando el separador adecuado.  \n",
    "\n",
    "### 2. Tratamiento de valores *unknown*\n",
    "- En las variables categ√≥ricas se detectaron valores `\"unknown\"`.  \n",
    "  ‚úÖ Se mantienen como categor√≠as v√°lidas para no perder informaci√≥n.  \n",
    "  ‚ö†Ô∏è Se evaluar√° m√°s adelante si conviene recodificarlos o eliminarlos seg√∫n su frecuencia.\n",
    "\n",
    "### 3. Eliminaci√≥n de columnas irrelevantes\n",
    "- En el dataset **customers** se elimin√≥ la columna `Unnamed: 0`, ya que solo era un √≠ndice sin valor anal√≠tico.  \n",
    "- No se identificaron m√°s columnas redundantes o vac√≠as.  \n",
    "\n",
    "### 4. Variables derivadas de fechas\n",
    "- En el dataset **customers** se trabaj√≥ con la variable `dt_customer`.  \n",
    "  ‚úÖ Se gener√≥ una nueva columna con la **antig√ºedad del cliente en d√≠as**, √∫til para an√°lisis temporales.  \n",
    "\n",
    "### 5. Detecci√≥n de outliers\n",
    "- Se revisaron las variables num√©ricas de ambos datasets.  \n",
    "  ‚úÖ No se detectaron outliers cr√≠ticos que requirieran eliminaci√≥n inmediata.  \n",
    "  ‚ö†Ô∏è En fases posteriores (modelado o visualizaci√≥n) se profundizar√° en su impacto.\n",
    "\n",
    "### 6. Normalizaci√≥n de texto en categ√≥ricas\n",
    "- Se estandariz√≥ el formato de texto en las variables categ√≥ricas (min√∫sculas y sin espacios extra).  \n",
    "  ‚úÖ Esto evita duplicidades del tipo `\"Yes\"` y `\"yes\"`.  \n",
    "\n",
    "### 7. Columnas con baja varianza\n",
    "- Se verific√≥ la varianza de las columnas categ√≥ricas.  \n",
    "  ‚úÖ Ninguna columna present√≥ baja varianza.  \n",
    "  ‚û°Ô∏è No fue necesario eliminar ninguna.  \n",
    "\n",
    "---\n",
    "\n",
    "üìå Con estas acciones, los datasets est√°n **limpios, unificados y normalizados**, listos para pasar a la fase de **EDA exploratorio**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c451b815",
   "metadata": {},
   "source": [
    "## GARUDAMOS NUESTROS ARCHIVOS LIMPIOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ce36bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Archivo guardado: customer-details_limpio.xlsx\n",
      "‚úÖ Archivo guardado: bank-additional_limpio.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Definir ruta destino\n",
    "ruta_guardado = r\"C:\\Users\\HUGO\\Desktop\\Data Analyst\\09_PYTHON\\Proyecto_Python_Data\\data\\Archivos_limpios\"\n",
    "\n",
    "# Guardar versi√≥n limpia del dataset customers\n",
    "df_customer.to_excel(os.path.join(ruta_guardado, \"customer-details_limpio.xlsx\"), index=False)\n",
    "print(\"‚úÖ Archivo guardado:\", \"customer-details_limpio.xlsx\")\n",
    "\n",
    "# Guardar versi√≥n limpia del dataset bank\n",
    "df_bank.to_csv(os.path.join(ruta_guardado, \"bank-additional_limpio.csv\"), index=False, encoding=\"utf-8\")\n",
    "print(\"‚úÖ Archivo guardado:\", \"bank-additional_limpio.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
